{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2yFgrTdGTXb"
      },
      "source": [
        "# Caso Práctico\n",
        "\n",
        "* Antes de procesar los texto con cualquier algoritmo de aprendizaje automático (supervisado o no supervisado) es necesario realizar un preporcesamiento con el objetivo de limpiar, normalizar y estructurar el texto.\n",
        "\n",
        "\n",
        "* Para ello se propone el siguiente framework:\n",
        "\n",
        "\n",
        "* Los pasos propuestos en este framework pueden abordarse en el orden que se quiera e incluso alguno de estas etapas no sería necesario realizarse en función de como tengamos los textos.\n",
        "\n",
        "\n",
        "* Definamos a continuación lo que hay que realizar en cada uno de estos pasos:\n",
        "\n",
        "\n",
        "1.- ***Eliminación de ruido***:\n",
        "\n",
        "   * Este paso tiene como objetivo eliminar todos aquellos símbolos o caracteres que no aportan nada en el significado de las frases (ojo no confundir con las stop-words), como por ejemplo etiquetas HTML (para el caso del scraping), parseos de XML, JSON, etc.\n",
        "    \n",
        "2.- ***Tokenización***:\n",
        "   * Este paso tiene como objetivo dividir las cadenas de texto del documento en piezas más pequeñas o tokens.\n",
        "   * Aunque la tokenización es el proceso de dividir grandes cadenas de texto en cadenas más pequeñas, se suele diferenciar la:\n",
        "       * ***Segmentation***: Tarea de dividir grandes cadenas de texto en piezas más pequeñas como oraciones o párrafos.\n",
        "       * ***Tokenization***: Tarea de dividir grandes cadenas de texto solo y exclusivamente en palabras.\n",
        "    \n",
        "3.- ***Normalización***:\n",
        "\n",
        "   * La normalización es una tarea que tiene como objetivo poner todo el texto en igualdad de condiciones:\n",
        "        * Convertir todo el texto en mayúscula o minúsculas\n",
        "        * Eliminar, puntos, comas, comillas, etc.\n",
        "        * Convertir los números a su equivalente a palabras\n",
        "        * Quitar las Stop-words\n",
        "        * etc.\n",
        "        \n",
        "<hr>\n",
        "\n",
        "## Ejemplo de Preprocesamiento de Texto.\n",
        "\n",
        "\n",
        "* Aunque no hay una norma o guía de como realizar una normalización de texto ya que esta depende del problema a resolver y de la naturaleza del texto, vamos a mostrar a continuación algunas operaciones más o menos comúnes para la tokenización y normalización de los textos.\n",
        "\n",
        "\n",
        "* En el siguiente ejemplo vamos a tokenizar y normalizar un texto:\n",
        "    1. Transformar un texto en tokens\n",
        "    2. Eliminar los tokens que son signos (puntuación, exclamación, etc.)\n",
        "    3. Eliminar las palabras que tienen menos de 'N' caracteres\n",
        "    4. Eliminar las palabras que son Stop Words\n",
        "    5. Pasar el texto a minúsculas\n",
        "    6. Lematización\n",
        "    \n",
        "    \n",
        "* **Nota**: *la normalización de texto que se va a codificar a continuación puede codificarse de forma más optimizada sin la necesidad de recorrer tantas veces la lista de tokens. Ya que este es un ejemplo con fines didácticos, este se centra en los conceptos y no en la optimización*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install spacy"
      ],
      "metadata": {
        "id": "3eHhfUUTGa85"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m spacy download es"
      ],
      "metadata": {
        "id": "n8RZ0KJXGk0p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bgBAmYI1GTXc"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iaNgfdrnGTXc"
      },
      "outputs": [],
      "source": [
        "def get_tokens(text):\n",
        "    \"\"\"\n",
        "    Función que dado un texto devuelve una lista con las palabras del texto no vacias\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    return [word.text.strip() for word in doc if len(word.text.strip()) > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j4elcavyGTXc"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(words):\n",
        "    \"\"\"\n",
        "    Función que dada una lista de palabras, elimina los signos de puntuación\n",
        "    \"\"\"\n",
        "    doc = spacy.tokens.doc.Doc(nlp.vocab, words=words)\n",
        "    return [word.text for word in doc if not word.is_punct]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1LsP1SBUGTXc"
      },
      "outputs": [],
      "source": [
        "def remove_short_words(words, num_chars):\n",
        "    \"\"\"\n",
        "    Función que dada una lista de palabras y un número mínimo de caracteres que tienen que tener\n",
        "    las palabras, elimina todas las palabras que tengan menos caracteres que los indicados\n",
        "    \"\"\"\n",
        "    return [word for word in words if len(word) > num_chars]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_D1yRsG_GTXc"
      },
      "outputs": [],
      "source": [
        "def remove_stop_words(words):\n",
        "    \"\"\"\n",
        "    Función que dada una lista de palabras, elimina las Stop Words\n",
        "    \"\"\"\n",
        "    doc = spacy.tokens.doc.Doc(nlp.vocab, words=words)\n",
        "    return [word.text for word in doc if not word.is_stop]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_uppercase(words):\n",
        "  \"\"\"\n",
        "  Función que dada una lista de palabaras, las transforma a mayusculas\n",
        "  \"\"\"\n",
        "  return [word.upper() for word in words]"
      ],
      "metadata": {
        "id": "23bEhCRHI0Eo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CVmSAZRWGTXc"
      },
      "outputs": [],
      "source": [
        "def to_lowercase(words):\n",
        "    \"\"\"\n",
        "    Función que dada una lista de palabras, las transforma a minúsculas\n",
        "    \"\"\"\n",
        "    return [word.lower() for word in words]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizer(text):\n",
        "    \"\"\"\n",
        "    Función que dado un texto, devuelve una lista de lemas de las palabras en el texto\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]"
      ],
      "metadata": {
        "id": "xqNzAmRQfh2J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "    \"\"\"\n",
        "    Dado un texto, devuelve el texto tokenizado y normalizado\n",
        "    \"\"\"\n",
        "    words = get_tokens(text=text)\n",
        "    words = remove_punctuation(words=words)\n",
        "    words = remove_short_words(words=words, num_chars=3)\n",
        "    words = remove_stop_words(words)\n",
        "    words = to_uppercase(words)\n",
        "    words = to_lowercase(words)\n",
        "    lemmatized_text = \" \".join(words)  # Unir las palabras en una cadena de texto\n",
        "    lemmatized_words = lemmatizer(lemmatized_text)  # Aplicar lemmatizer al texto unido\n",
        "    return lemmatized_words"
      ],
      "metadata": {
        "id": "cWFuA42qfxa4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhdsutF4GTXc"
      },
      "outputs": [],
      "source": [
        "#def lemmatizer(words):\n",
        "    \"\"\"\n",
        "    Función que dada una lista de palabras, devuelve esa lista con el lema de cada una de esas palabras\n",
        "    \"\"\"\n",
        "    doc = spacy.tokens.doc.Doc(nlp.vocab, words=words)\n",
        "    return [word.lemma_ for word in doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "UCD2OkLKGTXd"
      },
      "outputs": [],
      "source": [
        "#def normalize(text):\n",
        "    \"\"\"\n",
        "    Dado un texto, devuelve el texto tokenizado y normalizado\n",
        "    \"\"\"\n",
        "    words = get_tokens(text=text)\n",
        "    words = remove_punctuation(words=words)\n",
        "    words = remove_short_words(words=words, num_chars=3)\n",
        "    words = remove_stop_words(words)\n",
        "    words = to_uppercase(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = lemmatizer(words)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSxEFDUWGTXd"
      },
      "source": [
        "#### Pasamos a tokenizar y normalizar el siguiente texto usando la función de normalización realizada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5np0itkGTXd",
        "outputId": "bf69d200-de63-40d9-bdbb-fdc8f3b81a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lionel', 'andrés', 'messi', 'cuccittini', 'conocido', 'messi', 'futbolista', 'argentino', 'jugar', 'delantero', 'centrocampista', '2023', 'integrar', 'plantel', 'inter', 'miami', 'canadoestadounidense', 'internacional', 'selección', 'argentino', 'capitán']\n"
          ]
        }
      ],
      "source": [
        "raw = \"\"\"Lionel Andrés Messi Cuccittini, conocido como Leo Messi, es un futbolista argentino que juega como delantero o centrocampista. Desde 2023, integra el plantel del Inter Miami de la MLS canadoestadounidense. Es también internacional con la selección de Argentina, de la que es capitán.\"\"\"\n",
        "print(normalize(raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9jQqXdxGTXd"
      },
      "source": [
        "#### En este ejemplo podemos ver como reducimos las palabras (tokens) del texto original, quedandonos con lo importante y normalizado\n",
        "#### Pasamos de 128 tokens del texto original a 44 tokens tras la normalización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp-gv6QXGTXd",
        "outputId": "979cadfd-9c88-4fcb-f8b1-ea652fef0db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens del texto original: 50\n",
            "Número de tokens distintos del texto original: 37\n",
            "Número de tokens tras la normalización: 21\n",
            "Número de tokens distintos tras la normalización: 19\n"
          ]
        }
      ],
      "source": [
        "print('Número de tokens del texto original: ' + str(len(get_tokens(raw))))\n",
        "print('Número de tokens distintos del texto original: ' + str(len(set(get_tokens(raw)))))\n",
        "print('Número de tokens tras la normalización: ' + str(len(normalize(raw))))\n",
        "print('Número de tokens distintos tras la normalización: ' + str(len(set(normalize(raw)))))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}